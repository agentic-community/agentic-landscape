[{"category":"LLMs","homepage_url":"https://www.sglang.io/","id":"llms--serving--sglang","logo_url":"http://127.0.0.1:8000/logos/c801ac352c84d27689070962d2d8bf4d042c369759bac9dfa40d89eac48c027e.png","name":"SGLang","subcategory":"Serving","description":"SGLang is a high-performance serving framework for large language models and multimodal models.","oss":true,"repositories":[{"url":"https://github.com/sgl-project/sglang","languages":{"C":128264,"C++":1382831,"CMake":31951,"Cuda":1683302,"Dockerfile":80707,"Go":105506,"HIP":15490,"Jinja":1205,"Jupyter Notebook":7306,"Makefile":23168,"Python":25582937,"Rust":3035844,"Shell":158702,"Vim Script":914},"primary":true}]},{"category":"LLMs","homepage_url":"https://vllm.ai/","id":"llms--serving--vllm","logo_url":"http://127.0.0.1:8000/logos/789d223c86d7884c48ea51f8e799e8c0ce952495186131fcaed24c06292d0e45.svg","name":"vLLM","subcategory":"Serving","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","languages":{"C":95214,"C++":1260879,"CMake":98450,"Cuda":2024623,"Dockerfile":35685,"HCL":1731,"Jinja":6380,"Python":26372976,"Shell":258453},"primary":true}]}]